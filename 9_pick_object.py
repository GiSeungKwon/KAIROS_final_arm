import torch
import torch.nn as nn
from torchvision import models, transforms
import numpy as np
import os
import cv2
import time
import sys
from pymycobot.mycobot import MyCobot

# ----------------------------------------------------
# 0. í™˜ê²½ ì„¤ì • ë° ìƒìˆ˜ (Configuration & Constants)
# ----------------------------------------------------
# í›ˆë ¨ ì‹œ ì„¤ì •ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€
MODEL_PATH = os.path.join("models", "best_model.pth") # ì €ì¥ëœ Best Model ê²½ë¡œ
PORT = "COM3"
BAUD = 115200
CAMERA_INDEX = 0

# í›ˆë ¨ ì‹œ ì‚¬ìš©ëœ ê´€ì ˆ ì œí•œ (Normalizationì„ ìœ„í•œ Min/Max)
# ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ëœ ê²ƒì„ í™œìš©í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
JOINT_MIN = np.array([-165.0, -165.0, -165.0, -165.0, -165.0, -175.0], dtype=np.float32)
JOINT_MAX = np.array([ 165.0, 165.0, 165.0, 165.0, 165.0, 175.0], dtype=np.float32)
JOINT_RANGE = JOINT_MAX - JOINT_MIN

# ROI ì˜ì—­ (í›ˆë ¨ ì‹œì™€ ë™ì¼)
ROI_START = (30, 30) # (x_min, y_min)
ROI_END = (430, 430) # (x_max, y_max)

# ë¡œë´‡ ë™ì‘ ì„¤ì •
MOVEMENT_SPEED = 30 
GRIPPER_SPEED = 20 
SEQUENTIAL_MOVE_DELAY = 1 
GRIPPER_ACTION_DELAY = 1 

# ë¡œë´‡ ìì„¸ ì„¤ì •
INTERMEDIATE_POSE_ANGLES = [-17.2, 30.49, 4.48, 53.08, -90.87, -85.86] # ê²½ìœ  ìì„¸
CONVEYOR_CAPTURE_POSE = [0, 0, 90, 0, -90, -90] # 1 í‚¤ ìì„¸
ROBOTARM_CAPTURE_POSE = [0, 0, 90, 0, -90, 90] # 2 í‚¤ ìì„¸
ZERO_POSE_ANGLES = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
GRIPPER_OPEN_VALUE = 55 
GRIPPER_CLOSED_VALUE = 25 

# CUDA ì„¤ì •
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"âœ… ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

# ResNet í‘œì¤€ ì •ê·œí™” ìƒìˆ˜
MEAN = [0.485, 0.456, 0.406]
STD = [0.229, 0.224, 0.225]

# ----------------------------------------------------
# 1. ëª¨ë¸ ì •ì˜ ë° ë¡œë“œ (Model Definition and Loading)
# ----------------------------------------------------
def load_model(model_path):
    """ResNet-50 ëª¨ë¸ì„ ì •ì˜í•˜ê³  ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    # 1. ëª¨ë¸ êµ¬ì¡° ì •ì˜ (í›ˆë ¨ ì½”ë“œì™€ ë™ì¼í•´ì•¼ í•¨)
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    num_ftrs = model.fc.in_features
    # ì¶œë ¥: Joint 1 ~ Joint 6 (ì´ 6ê°œ)
    model.fc = nn.Linear(num_ftrs, 6)

    # 2. ê°€ì¤‘ì¹˜ ë¡œë“œ
    try:
        model.load_state_dict(torch.load(model_path, map_location=DEVICE))
        model = model.to(DEVICE)
        model.eval() # ì¶”ë¡  ëª¨ë“œ ì„¤ì •
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        return model
    except FileNotFoundError:
        print(f"\nâŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("í›ˆë ¨ì„ ë¨¼ì € ì§„í–‰í•˜ì—¬ ëª¨ë¸ íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

# ----------------------------------------------------
# 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ì—­ì •ê·œí™” (Preprocessing & Denormalization)
# ----------------------------------------------------
def preprocess_image(image, roi_start, roi_end):
    """
    OpenCV ì´ë¯¸ì§€ë¥¼ í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ PyTorch ì „ì²˜ë¦¬(Resize, ToTensor, Normalize)ì™€ ë™ì¼í•˜ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
    :param image: ì›ë³¸ OpenCV BGR ì´ë¯¸ì§€
    :param roi_start: ROI ì‹œì‘ì  (x_min, y_min)
    :param roi_end: ROI ëì  (x_max, y_max)
    :return: ì „ì²˜ë¦¬ëœ PyTorch Tensor (batch=1)
    """
    
    # 1. ROI í¬ë¡­ (í›ˆë ¨ ë°ì´í„°ì…‹ì´ masked_outputì—ì„œ ì™”ìœ¼ë¯€ë¡œ í¬ë¡­ ìˆ˜í–‰)
    x_min, y_min = roi_start
    x_max, y_max = roi_end
    
    # OpenCV ì´ë¯¸ì§€ëŠ” (H, W, C)
    # ì´ë¯¸ì§€ ë°°ì—´ ì¸ë±ì‹±ì€ [y_min:y_max, x_min:x_max] ìˆœì„œ
    cropped_image = image[y_min:y_max, x_min:x_max]

    # 2. BGR -> RGB ë³€í™˜ (í›ˆë ¨ ì½”ë“œì˜ MyCobotDatasetê³¼ ë™ì¼)
    image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)
    
    # 3. PyTorch Transforms ì ìš©
    # ToPILImage() -> Resize(224, 224) -> ToTensor() -> Normalize()
    transform = transforms.Compose([
        transforms.ToPILImage(), # numpy arrayë¥¼ PIL Imageë¡œ ë³€í™˜
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=MEAN, std=STD)
    ])

    # 
    
    input_tensor = transform(image_rgb)
    
    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1, C, H, W)
    input_batch = input_tensor.unsqueeze(0).to(DEVICE)
    return input_batch, image_rgb # ì „ì²˜ë¦¬ í›„ì˜ RGB ì´ë¯¸ì§€ë„ ë°˜í™˜í•˜ì—¬ í™”ë©´ì— í‘œì‹œ

def denormalize_angles(normalized_angles):
    """
    ëª¨ë¸ ì¶œë ¥ê°’([-1, 1])ì„ ì‹¤ì œ ê´€ì ˆ ê°ë„(Joint Angle, [Min, Max])ë¡œ ì—­ì •ê·œí™”í•©ë‹ˆë‹¤.
    :param normalized_angles: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì •ê·œí™”ëœ ê°ë„ (Shape: (6,))
    :return: ì‹¤ì œ Joint Angle (Deg, Shape: (6,))
    """
    # ê³µì‹: Angle = (Normalized_Angle + 1) / 2 * (Max - Min) + Min
    # Angle = (Normalized_Angle + 1) / 2 * JOINT_RANGE + JOINT_MIN
    
    normalized_angles_np = normalized_angles.cpu().detach().numpy()
    
    # [-1, 1] -> [0, 1]
    y_prime = (normalized_angles_np + 1.0) / 2.0
    
    # [0, 1] -> [Min, Max]
    actual_angles = y_prime * JOINT_RANGE + JOINT_MIN
    
    # MyCobotì€ ê°ë„ë¥¼ ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ë§Œ ë°›ìœ¼ë¯€ë¡œ ë°˜ì˜¬ë¦¼
    actual_angles = np.round(actual_angles, 1)
    
    return actual_angles.tolist()

# ----------------------------------------------------
# 3. ì¶”ë¡  ë° ë¡œë´‡ ì œì–´ í•¨ìˆ˜ (Inference and Robot Control)
# ----------------------------------------------------
def infer_and_move(mc, model, inputs):
    """
    ëª¨ë¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³ , ë¡œë´‡ì„ ì¶”ë¡ ëœ ìì„¸ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    """
    try:
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model(inputs)
        
        # ì¶œë ¥: (1, 6) -> (6,)
        normalized_angles = outputs.squeeze(0) 
        
        # ì—­ì •ê·œí™”
        target_angles = denormalize_angles(normalized_angles)
        
        print("\nâœ¨ ì¶”ë¡  ê²°ê³¼:")
        print(f"  > ì •ê·œí™”ëœ ì˜ˆì¸¡ê°’: {normalized_angles.tolist()}")
        print(f"  > ìµœì¢… ê´€ì ˆ ê°ë„(Deg): {target_angles}")
        
        # ë¡œë´‡ ì´ë™ ë¡œì§
        # 1. ê²½ìœ  ìì„¸ë¡œ ì´ë™ (ì•ˆì „ ê²½ë¡œ í™•ë³´)
        mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
        time.sleep(SEQUENTIAL_MOVE_DELAY)
        
        # 2. ëª©í‘œ ìì„¸(ì¶”ë¡  ê²°ê³¼)ë¡œ ì´ë™
        mc.send_angles(target_angles, MOVEMENT_SPEED)
        print("âœ… ë¡œë´‡ íŒ” ëª©í‘œ ìì„¸ë¡œ ì´ë™ ì™„ë£Œ.")
        
        return target_angles
        
    except Exception as e:
        print(f"\nâŒ ì¶”ë¡  ë° ë¡œë´‡ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# ----------------------------------------------------
# 4. ë©”ì¸ ë£¨í”„ (Main Loop)
# ----------------------------------------------------
def main():
    # 1. MyCobot ë° ëª¨ë¸ ë¡œë“œ
    model = load_model(MODEL_PATH)
    try:
        mc = MyCobot(PORT, BAUD)
        mc.power_on() 
        mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
        print(f"\nğŸ¤– MyCobot ì—°ê²° ì„±ê³µ: {PORT}. ê·¸ë¦¬í¼ Open.")

    except Exception as e:
        print(f"\nâŒ MyCobot ì—°ê²° ì‹¤íŒ¨ ({PORT}): {e}")
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)

    # 2. ì¹´ë©”ë¼ ì—°ê²°
    cap = cv2.VideoCapture(CAMERA_INDEX, cv2.CAP_DSHOW)
    if not cap.isOpened():
        print(f"\nâŒ ì¹´ë©”ë¼ ì¸ë±ìŠ¤ {CAMERA_INDEX}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        mc.close()
        sys.exit(1)
        
    last_inferred_angles = None # ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ë¡ ëœ ê°ë„ ì €ì¥

    # ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
    print("\n--- ğŸš€ MyCobot ì¶”ë¡  ë° ì œì–´ ë„êµ¬ ì‚¬ìš©ë²• ---")
    print(" Â [p] : í˜„ì¬ ROI ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜/ì „ì²˜ë¦¬í•˜ê³ , ëª¨ë¸ì— **ì¶”ë¡ (Inference)**ì„ ìš”ì²­í•©ë‹ˆë‹¤.")
    print(" Â [e] : ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ë¡ ëœ ê´€ì ˆ ê°ë„ë¡œ ë¡œë´‡ íŒ”ì„ **ì´ë™(Execute)**ì‹œí‚µë‹ˆë‹¤.")
    print(" Â [0] : ëª¨ë“  ê´€ì ˆì„ [0, 0, 0, 0, 0, 0] ìì„¸ë¡œ ì´ë™")
    print(" Â [1] : (ê²½ìœ ì§€ ê²½ìœ  í›„) CONVEYOR_CAPTURE_POSE ì´ë™ ë° ê³ ì •")
    print(" Â [2] : ROBOTARM_CAPTURE_POSE ì´ë™ ë° ê³ ì •")
    print(" Â [g] : ê·¸ë¦¬í¼ ë‹«ê¸°") 
    print(" Â [h] : ê·¸ë¦¬í¼ ì—´ê¸°") 
    print(" Â [q] : í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("---------------------------------------")


    while True:
        ret, frame = cap.read()
        if not ret:
            print("í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨. ì¹´ë©”ë¼ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.", end='\r')
            time.sleep(0.1)
            continue
        
        # 1. ë¼ì´ë¸Œ í™”ë©´ì— ROI í‘œì‹œ
        cv2.rectangle(frame, ROI_START, ROI_END, (0, 255, 0), 2) # ì´ˆë¡ìƒ‰ (Green)
        cv2.putText(frame, "ROI (Region of Interest)", (ROI_START[0], ROI_START[1]-10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

        # ìƒíƒœ í‘œì‹œ
        if last_inferred_angles:
            status_text = f"STATUS: Inferenced. Press 'e' to MOVE to {last_inferred_angles[0]}..."
            color = (0, 0, 255) # ë¹¨ê°„ìƒ‰: ì´ë™ ëŒ€ê¸° ì¤‘
        else:
            status_text = "STATUS: Ready. Press 'p' to Infer."
            color = (255, 255, 255) # í°ìƒ‰: ì¶”ë¡  ëŒ€ê¸° ì¤‘

        cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        cv2.imshow('MyCobot Live Camera', frame)

        key = cv2.waitKey(1) & 0xFF

        # [q]: í”„ë¡œê·¸ë¨ ì¢…ë£Œ
        if key == ord('q'):
            print("\nğŸ‘‹ end...")
            break
        
        # [p]: ì¶”ë¡ (Inference) ì‹¤í–‰ ë° ì „ì²˜ë¦¬ ì´ë¯¸ì§€ í‘œì‹œ
        elif key == ord('p'):
            print("\nğŸ” 'p' í‚¤ ì…ë ¥: ì¶”ë¡  ì‹œì‘.")
            # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_batch, preprocessed_rgb_img = preprocess_image(frame, ROI_START, ROI_END)

            # 2. ì „ì²˜ë¦¬ ì™„ë£Œëœ ì´ë¯¸ì§€ í‘œì‹œ (224x224)
            # RGB -> BGR ë³€í™˜í•˜ì—¬ OpenCVë¡œ í‘œì‹œ
            display_img = cv2.cvtColor(preprocessed_rgb_img, cv2.COLOR_RGB2BGR)
            # í‘œì‹œë¥¼ ìœ„í•´ í¬ê¸° ì¡°ì • (ì„ íƒ ì‚¬í•­, ì›ë³¸ í¬ê¸° 224x224)
            display_img = cv2.resize(display_img, (300, 300), interpolation=cv2.INTER_NEAREST) 
            cv2.imshow('Preprocessed Input (224x224)', display_img)
            
            print("ğŸ–¼ï¸ ì „ì²˜ë¦¬ ì´ë¯¸ì§€ í‘œì‹œ ì™„ë£Œ.")

            # 3. ëª¨ë¸ ì¶”ë¡ 
            inferred_angles = infer_and_move(mc, model, input_batch)
            if inferred_angles:
                # 'e' í‚¤ë¥¼ ëˆ„ë¥¼ ë•Œ ì´ë™í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
                last_inferred_angles = inferred_angles 

        # [e]: ë§ˆì§€ë§‰ ì¶”ë¡  ìì„¸ë¡œ ì´ë™ (ì‹¤ì œ ë¡œë´‡ ë™ì‘)
        # 'p' í‚¤ë¥¼ ëˆŒëŸ¬ ì¶”ë¡ ì„ ì™„ë£Œí•œ í›„, 'e' í‚¤ë¥¼ ëˆŒëŸ¬ ë¡œë´‡ì„ í•´ë‹¹ ìœ„ì¹˜ë¡œ ì´ë™.
        elif key == ord('e'):
            if last_inferred_angles:
                print(f"\nğŸš€ 'e' í‚¤ ì…ë ¥: ë§ˆì§€ë§‰ ì¶”ë¡  ìœ„ì¹˜({last_inferred_angles})ë¡œ ì´ë™ ì‹œì‘.")
                
                # 1. ê²½ìœ  ìì„¸ë¡œ ì´ë™ 
                mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
                time.sleep(SEQUENTIAL_MOVE_DELAY)

                # 2. ëª©í‘œ ìì„¸ë¡œ ì´ë™
                mc.send_angles(last_inferred_angles, MOVEMENT_SPEED)
                print("âœ… ë¡œë´‡ íŒ” ëª©í‘œ ìì„¸ë¡œ ì´ë™ ì™„ë£Œ.")
                
            else:
                print("\nâš ï¸ ë¨¼ì € 'p' í‚¤ë¥¼ ëˆŒëŸ¬ ê´€ì ˆ ê°ë„ë¥¼ ì¶”ë¡ (Infer)í•´ì•¼ í•©ë‹ˆë‹¤.")

        # [0], [1], [2], [g], [h] í‚¤ ì²˜ë¦¬ (ë¡œë´‡ ì œì–´)
        elif key == ord('0'):
            print(f"\nâš™ï¸ ZERO_POSE ì´ë™ ì‹œì‘.")
            mc.send_angles(CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            mc.send_angles(ZERO_POSE_ANGLES, MOVEMENT_SPEED)
            print("âœ… ZERO_POSE ì´ë™ ì™„ë£Œ.")
        
        elif key == ord('1'):
            print(f"\nğŸ  CONVEYOR_CAPTURE_POSE ì´ë™ ì‹œì‘.")
            mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
            mc.send_angles(INTERMEDIATE_POSE_ANGLES, MOVEMENT_SPEED)
            time.sleep(SEQUENTIAL_MOVE_DELAY)
            mc.send_angles(CONVEYOR_CAPTURE_POSE, MOVEMENT_SPEED)
            time.sleep(5)
            print("âœ… CONVEYOR_CAPTURE_POSE ì´ë™ ì™„ë£Œ.")

        elif key == ord('2'):
            print(f"\nğŸ  ROBOTARM_CAPTURE_POSE ì´ë™ ì‹œì‘.")
            mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
            mc.send_angles(ROBOTARM_CAPTURE_POSE, MOVEMENT_SPEED)
            time.sleep(5)
            print("âœ… ROBOTARM_CAPTURE_POSE ì´ë™ ì™„ë£Œ.")

        elif key == ord('g'):
            print("\nâœŠ ê·¸ë¦¬í¼ ë‹«ëŠ” ì¤‘...")
            mc.set_gripper_value(GRIPPER_CLOSED_VALUE, GRIPPER_SPEED) 
            time.sleep(GRIPPER_ACTION_DELAY)
            print(f"âœ… ê·¸ë¦¬í¼ ë‹«í˜ ì™„ë£Œ (ìœ„ì¹˜: {GRIPPER_CLOSED_VALUE}).")
            
        elif key == ord('h'):
            print("\nğŸ‘ ê·¸ë¦¬í¼ ì—¬ëŠ” ì¤‘...")
            mc.set_gripper_value(GRIPPER_OPEN_VALUE, GRIPPER_SPEED)
            time.sleep(GRIPPER_ACTION_DELAY)
            print(f"âœ… ê·¸ë¦¬í¼ ì—´ë¦¼ ì™„ë£Œ (ìœ„ì¹˜: {GRIPPER_OPEN_VALUE}).")


    # ì¢…ë£Œ ì •ë¦¬ ì‘ì—…
    cap.release()
    cv2.destroyAllWindows()
    try:
        mc.close()
    except Exception:
        pass

if __name__ == "__main__":
    main()